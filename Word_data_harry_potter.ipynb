{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word data harry potter.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN+E/8RJXKrmIo7E9WoVICW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mradzikowska/mbcs-internship/blob/main/Word_data_harry_potter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fMg8BQjnOdZ"
      },
      "source": [
        "import numpy as np\n",
        "from google.colab import drive\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWKENFA1oS7-",
        "outputId": "da31a4b1-08db-4017-d5fc-1eb61e717c0a"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVQZpLQeogYW"
      },
      "source": [
        "word_data = np.load ('/content/drive/My Drive/stimuli_words.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHVKcSB4q1IB",
        "outputId": "bfbc39bf-96f2-4a40-beea-048b2221bd2c"
      },
      "source": [
        "print(word_data)\n",
        "len(word_data)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Harry' 'had' 'never' ... 'thirteen' 'was.' '+']\n",
            "(5176,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6FgwER-snLI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "gS8o4deWsmj6"
      },
      "source": [
        "#@title Toneva's Code bert representations\n",
        "def get_bert_layer_representations(seq_len, text_array, remove_chars, word_ind_to_extract):\n",
        "\n",
        "    model = BertModel.from_pretrained('bert-base-uncased')\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    model.eval()\n",
        "\n",
        "    # get the token embeddings\n",
        "    token_embeddings = []\n",
        "    for word in text_array:\n",
        "        current_token_embedding = get_bert_token_embeddings([word], tokenizer, model, remove_chars)\n",
        "        token_embeddings.append(np.mean(current_token_embedding.detach().numpy(), 1))\n",
        "    \n",
        "    # where to store layer-wise bert embeddings of particular length\n",
        "    BERT = {}\n",
        "    for layer in range(12):\n",
        "        BERT[layer] = []\n",
        "    BERT[-1] = token_embeddings\n",
        "\n",
        "    if word_ind_to_extract < 0: # the index is specified from the end of the array, so invert the index\n",
        "        from_start_word_ind_to_extract = seq_len + 2 + word_ind_to_extract  # add 2 for CLS + SEP tokens\n",
        "    else:\n",
        "        from_start_word_ind_to_extract = word_ind_to_extract\n",
        "\n",
        "    start_time = tm.time()    \n",
        "        \n",
        "    # before we've seen enough words to make up the sequence length, add the representation for the last word 'seq_len' times\n",
        "    word_seq = text_array[:seq_len]\n",
        "    for _ in range(seq_len):\n",
        "        BERT = add_avrg_token_embedding_for_specific_word(word_seq,\n",
        "                                                                     tokenizer,\n",
        "                                                                     model,\n",
        "                                                                     remove_chars,\n",
        "                                                                     from_start_word_ind_to_extract,\n",
        "                                                                     BERT)\n",
        "\n",
        "    # then add the embedding of the last word in a sequence as the embedding for the sequence\n",
        "    for end_curr_seq in range(seq_len, len(text_array)):\n",
        "        word_seq = text_array[end_curr_seq-seq_len+1:end_curr_seq+1]\n",
        "        BERT = add_avrg_token_embedding_for_specific_word(word_seq,\n",
        "                                                          tokenizer,\n",
        "                                                          model,\n",
        "                                                          remove_chars,\n",
        "                                                          from_start_word_ind_to_extract,\n",
        "                                                          BERT)\n",
        "\n",
        "        if end_curr_seq % 100 == 0:\n",
        "            print('Completed {} out of {}: {}'.format(end_curr_seq, len(text_array), tm.time()-start_time))\n",
        "            start_time = tm.time()\n",
        "\n",
        "    print('Done extracting sequences of length {}'.format(seq_len))\n",
        "    \n",
        "    return BERT\n",
        "\n",
        "# extracts layer representations for all words in words_in_array\n",
        "# encoded_layers: list of tensors, length num layers. each tensor of dims num tokens by num dimensions in representation\n",
        "# word_ind_to_token_ind: dict that maps from index in words_in_array to index in array of tokens when words_in_array is tokenized,\n",
        "#                       with keys: index of word, and values: array of indices of corresponding tokens when word is tokenized\n",
        "def predict_bert_embeddings(words_in_array, tokenizer, model, remove_chars):    \n",
        "    \n",
        "    for word in words_in_array:\n",
        "        if word in remove_chars:\n",
        "            print('An input word is also in remove_chars. This word will be removed and may lead to misalignment. Proceed with caution.')\n",
        "            return -1\n",
        "    \n",
        "    n_seq_tokens = 0\n",
        "    seq_tokens = []\n",
        "    \n",
        "    word_ind_to_token_ind = {}             # dict that maps index of word in words_in_array to index of tokens in seq_tokens\n",
        "    \n",
        "    for i,word in enumerate(words_in_array):\n",
        "        word_ind_to_token_ind[i] = []      # initialize token indices array for current word\n",
        "\n",
        "        if word in ['[CLS]', '[SEP]']:     # [CLS] and [SEP] are already tokenized\n",
        "            word_tokens = [word]\n",
        "        else:    \n",
        "            word_tokens = tokenizer.tokenize(word)\n",
        "            \n",
        "        for token in word_tokens:\n",
        "            if token not in remove_chars:  # don't add any tokens that are in remove_chars\n",
        "                seq_tokens.append(token)\n",
        "                word_ind_to_token_ind[i].append(n_seq_tokens)\n",
        "                n_seq_tokens = n_seq_tokens + 1\n",
        "    \n",
        "    # convert token to vocabulary indices\n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(seq_tokens)\n",
        "    tokens_tensor = torch.tensor([indexed_tokens])\n",
        "    \n",
        "    encoded_layers, _ = model(tokens_tensor)\n",
        "    pooled_output = np.squeeze(model.pooler(encoded_layers[-1]).detach().numpy())\n",
        "    \n",
        "    return encoded_layers, word_ind_to_token_ind, pooled_output\n",
        "  \n",
        "# add the embeddings for a specific word in the sequence\n",
        "# token_inds_to_avrg: indices of tokens in embeddings output to avrg\n",
        "def add_word_bert_embedding(bert_dict, embeddings_to_add, token_inds_to_avrg, specific_layer=-1):\n",
        "    if specific_layer >= 0:  # only add embeddings for one specified layer\n",
        "        layer_embedding = embeddings_to_add[specific_layer]\n",
        "        full_sequence_embedding = layer_embedding.detach().numpy()\n",
        "        bert_dict[specific_layer].append(np.mean(full_sequence_embedding[0,token_inds_to_avrg,:],0))\n",
        "    else:\n",
        "        for layer, layer_embedding in enumerate(embeddings_to_add):\n",
        "            full_sequence_embedding = layer_embedding.detach().numpy()\n",
        "            bert_dict[layer].append(np.mean(full_sequence_embedding[0,token_inds_to_avrg,:],0)) # avrg over all tokens for specified word\n",
        "    return bert_dict\n",
        "\n",
        "# predicts representations for specific word in input word sequence, and adds to existing layer-wise dictionary\n",
        "#\n",
        "# word_seq: numpy array of words in input sequence\n",
        "# tokenizer: BERT tokenizer\n",
        "# model: BERT model\n",
        "# remove_chars: characters that should not be included in the represention when word_seq is tokenized\n",
        "# from_start_word_ind_to_extract: the index of the word whose features to extract, INDEXED FROM START OF WORD_SEQ\n",
        "# bert_dict: where to save the extracted embeddings\n",
        "def add_avrg_token_embedding_for_specific_word(word_seq,tokenizer,model,remove_chars,from_start_word_ind_to_extract,bert_dict):\n",
        "    \n",
        "    word_seq = ['[CLS]'] + list(word_seq) + ['[SEP]']   \n",
        "    all_sequence_embeddings, word_ind_to_token_ind, _ = predict_bert_embeddings(word_seq, tokenizer, model, remove_chars)\n",
        "    token_inds_to_avrg = word_ind_to_token_ind[from_start_word_ind_to_extract]\n",
        "    bert_dict = add_word_bert_embedding(bert_dict, all_sequence_embeddings,token_inds_to_avrg)\n",
        "    \n",
        "    return bert_dict\n",
        "\n",
        "\n",
        "# get the BERT token embeddings\n",
        "def get_bert_token_embeddings(words_in_array, tokenizer, model, remove_chars):    \n",
        "    for word in words_in_array:\n",
        "        if word in remove_chars:\n",
        "            print('An input word is also in remove_chars. This word will be removed and may lead to misalignment. Proceed with caution.')\n",
        "            return -1\n",
        "    \n",
        "    n_seq_tokens = 0\n",
        "    seq_tokens = []\n",
        "    \n",
        "    word_ind_to_token_ind = {}             # dict that maps index of word in words_in_array to index of tokens in seq_tokens\n",
        "    \n",
        "    for i,word in enumerate(words_in_array):\n",
        "        word_ind_to_token_ind[i] = []      # initialize token indices array for current word\n",
        "\n",
        "        if word in ['[CLS]', '[SEP]']:     # [CLS] and [SEP] are already tokenized\n",
        "            word_tokens = [word]\n",
        "        else:    \n",
        "            word_tokens = tokenizer.tokenize(word)\n",
        "            \n",
        "        for token in word_tokens:\n",
        "            if token not in remove_chars:  # don't add any tokens that are in remove_chars\n",
        "                seq_tokens.append(token)\n",
        "                word_ind_to_token_ind[i].append(n_seq_tokens)\n",
        "                n_seq_tokens = n_seq_tokens + 1\n",
        "    \n",
        "    # convert token to vocabulary indices\n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(seq_tokens)\n",
        "    tokens_tensor = torch.tensor([indexed_tokens])\n",
        "    \n",
        "    token_embeddings = model.embeddings.forward(tokens_tensor)\n",
        "    \n",
        "    return token_embeddings\n",
        "    \n",
        "    \n",
        "# add the embeddings for all individual words\n",
        "# specific_layer specifies only one layer to add embeddings from\n",
        "def add_all_bert_embeddings(bert_dict, embeddings_to_add, specific_layer=-1):\n",
        "    if specific_layer >= 0:\n",
        "        layer_embedding = embeddings_to_add[specific_layer]\n",
        "        seq_len = layer_embedding.shape[1]\n",
        "        full_sequence_embedding = layer_embedding.detach().numpy()\n",
        "        \n",
        "        for word in range(seq_len):\n",
        "            bert_dict[specific_layer].append(full_sequence_embedding[0,word,:])\n",
        "    else:  \n",
        "        for layer, layer_embedding in enumerate(embeddings_to_add):\n",
        "            seq_len = layer_embedding.shape[1]\n",
        "            full_sequence_embedding = layer_embedding.detach().numpy()\n",
        "\n",
        "            for word in range(seq_len):\n",
        "                bert_dict[layer].append(full_sequence_embedding[0,word,:])\n",
        "    return bert_dict\n",
        "\n",
        "\n",
        "# add the embeddings for only the last word in the sequence that is not [SEP] token\n",
        "def add_last_nonsep_bert_embedding(bert_dict, embeddings_to_add, specific_layer=-1):\n",
        "    if specific_layer >= 0:\n",
        "        layer_embedding = embeddings_to_add[specific_layer]\n",
        "        full_sequence_embedding = layer_embedding.detach().numpy()\n",
        "        \n",
        "        bert_dict[specific_layer].append(full_sequence_embedding[0,-2,:])\n",
        "    else:\n",
        "        for layer, layer_embedding in enumerate(embeddings_to_add):\n",
        "            full_sequence_embedding = layer_embedding.detach().numpy()\n",
        "\n",
        "            bert_dict[layer].append(full_sequence_embedding[0,-2,:])\n",
        "    return bert_dict\n",
        "\n",
        "# add the CLS token embeddings ([CLS] is the first token in each string)\n",
        "def add_cls_bert_embedding(bert_dict, embeddings_to_add, specific_layer=-1):\n",
        "    if specific_layer >= 0:\n",
        "        layer_embedding = embeddings_to_add[specific_layer]\n",
        "        full_sequence_embedding = layer_embedding.detach().numpy()\n",
        "        \n",
        "        bert_dict[specific_layer].append(full_sequence_embedding[0,0,:])\n",
        "    else:\n",
        "        for layer, layer_embedding in enumerate(embeddings_to_add):\n",
        "            full_sequence_embedding = layer_embedding.detach().numpy()\n",
        "\n",
        "            bert_dict[layer].append(full_sequence_embedding[0,0,:])\n",
        "    return bert_dict"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}